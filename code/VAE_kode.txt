#Kode fra https://medium.com/mlearning-ai/generating-artificial-tabular-data-with-an-encoder-decoder-5e4de9b4d02e

#Preparing data
import pandas as pd

df = pd.read_csv("../input/iris/Iris.csv")
print(df.head().to_markdown())


#For the sake of simplicity, I shorten the column and Species names and drop the id:
df.drop(["Id"], axis=1, inplace=True)
df["Species"] = df["Species"].map(
    {
        "Iris-setosa": "Setosa",
        "Iris-versicolor": "Versicolor",
        "Iris-virginica": "Virginica",
    }
)
df.columns = ["SepalLength", "SepalWidth", "PetalLength", "PetalWidth", "Species"]
print(df.head().to_markdown())

#In this example, I want to generate artificial data. 
#Thus, I treat the Species column as a categorical variable, and for simplicity I dummy-encode it:
df = pd.get_dummies(df)

#After the data preparation, I define a Neural Network that takes as input the seven columns of the data set, 
#contains sixteen neurons in a single hidden layer (“Hidden encoding”), and generates a single output z. 
#The latter is the internal state. Sampling from the (empirical) distribution of z, I will generate as many data 
#following the same distribution function as the input data.

#Encoder
from keras.layers import Input
from keras.layers import Dense
from keras.layers import BatchNormalization
from keras import backend as K


latent_dimension = 1
batch_size = 20
hidden_nodes = 16


input_encoder = Input(shape=(7,), name="Input_Encoder")
batch_normalize1 = BatchNormalization()(input_encoder)
hidden_layer = Dense(hidden_nodes, activation="relu", name="Hidden_Encoding")(
    batch_normalize1
)
batch_normalize2 = BatchNormalization()(hidden_layer)
z = Dense(latent_dimension, name="Mean")(batch_normalize2)

#This is the encoding component, so I conveniently name the model as encoder:

from keras import Model


encoder = Model(input_encoder, z)

#Then, I create the decoder
input_decoder = Input(shape=(latent_dimension,), name="Input_Decoder")
batch_normalize1 = BatchNormalization()(input_decoder)
decoder_hidden_layer = Dense(hidden_nodes, activation="relu", name="Hidden_Decoding")(
    batch_normalize1
)
batch_normalize2 = BatchNormalization()(decoder_hidden_layer)
decoded = Dense(7, activation="linear", name="Decoded")(batch_normalize2)

#And again, I name the decoder component for convenience:
decoder = Model(input_decoder, decoded, name="Decoder")

#The encoder/decoder is the composition of encoder and decoder:
encoder_decoder = decoder(encoder(input_encoder))

ae = Model(input_encoder, encoder_decoder)
ae.summary()


#Then I train the Neural Network. I deliberately do not fine-tune the hyper-parameters: 
#the intent is only to illustrate the results:

from tensorflow.random import set_seed


set_seed(2021)
ae.compile(loss="mean_squared_error", optimizer="adam")
history = ae.fit(
    df, df, shuffle=True, epochs=1000, batch_size=20, validation_split=0.2, verbose=0
).history

#Plot MSE as a function of the epochs, to see that the training converged:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


sns.set(font_scale=2)
sns.set_style("white")


def model_analysis(history):
    train_loss = history["loss"]
    val_loss = history["val_loss"]
    t = np.linspace(1, len(train_loss), len(train_loss))

    plt.figure(figsize=(16, 12))
    plt.title("Mean squared error")
    sns.lineplot(x=t, y=train_loss, label="Train", linewidth=3)
    sns.lineplot(x=t, y=val_loss, label="Validation", linewidth=3)
    plt.xlabel("Epochs")

    plt.legend()
    plt.savefig("FirstNet.png", dpi=400)
    plt.show()
    print(f"Training MSE = {np.sqrt(train_loss[-1])}")
    print(f"Validation MSE = {np.sqrt(val_loss[-1])}")


model_analysis(history)


#Plot the distribution function for the internal state z:
plt.figure(figsize=(16, 12))
plt.title("Empirical distribution function z")
plt.xticks((-5, -4, -3, -2, -1, 0, 1, 2, 3, 4))
plt.hist(encoder.predict(df), bins=30, density=True)
plt.savefig("DistInternal.png", dpi=400)

#Prediction on the first row of the training set
ae.predict(df)[0,:]

#Generating data
#Plot first empirical cumulative distribution function for the state z:
from statsmodels.distributions.empirical_distribution import ECDF


ecdf = ECDF(encoder.predict(df)[:, 0])
plt.figure(figsize=(16, 12))
plt.title("Empirical distribution function z")
x = (-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)
plt.yticks(np.linspace(0, 1, 11))
plt.xticks(x)
plt.grid()
plt.plot(x, ecdf(x), linewidth=3)
plt.savefig("EmpiricalDF.png", dpi=400)


#In order to sample the Distribution Function for z, since I don’t know a closed-form expression, 
#I invert the linear interpolation of the ECDF
from scipy.interpolate import interp1d


x = (-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)
sample_edf_values_at_slope_changes = [ecdf(i) for i in x]
inverted_edf = interp1d(sample_edf_values_at_slope_changes, x)

#I plot the inverted ECDF:
from numpy.random import uniform
from numpy.random import seed


N = 10000
seed(2021)
plt.figure(figsize=(16, 12))
plt.title("Inverted empirical distribution function")
x = np.linspace(0.05, 0.95, 30)
plt.xticks(np.linspace(0, 1.0, 11))
plt.grid()
plt.plot(x, inverted_edf(x), linewidth=3)
plt.savefig("InvertedEmpiricalDF.png", dpi=400)

#Plot the distribution function for the sampled z:
N = 10000
seed(2021)
plt.figure(figsize=(16, 12))
plt.title("Empirical distribution function z")
plt.xticks((-5, -4, -3, -2, -1, 0, 1, 2, 3, 4))
plt.hist(inverted_edf(uniform(0.05, 0.95, N)), bins=30, density=True)
plt.savefig("DistGenerated.png", dpi=400)


#Using the decoder, I can generate 10000 new observations that resemble the original Iris data set:
N = 10000


output = pd.DataFrame(decoder(inverted_edf(uniform(0.02, 0.95, N))).numpy())
output.columns = df.columns
output["SepalLength"] = [round(x, 1) for x in output["SepalLength"]]
output["SepalWidth"] = [round(x, 1) for x in output["SepalWidth"]]
output["PetalLength"] = [round(x, 1) for x in output["PetalLength"]]
output["PetalWidth"] = [round(x, 1) for x in output["PetalWidth"]]
output["Species"] = output.iloc[:, 4:8].apply(lambda x: dic[np.argmax(x)], axis=1)
output.drop(
    ["Species_Setosa", "Species_Versicolor", "Species_Virginica"], axis=1, inplace=True
)
print(output.head().to_markdown())